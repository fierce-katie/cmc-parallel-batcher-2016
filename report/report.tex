\documentclass[oneside,final,14pt]{extreport}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{vmargin}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{diagbox}
\newcommand*{\No}{\textnumero}
\setpapersize{A4}
\setmarginsrb{3cm}{2cm}{1cm}{2cm}{0pt}{0mm}{0pt}{13mm}
\usepackage{indentfirst}
\sloppy
\setcounter{page}{2}
\clubpenalty = 10000
\widowpenalty = 10000

\begin{document}
{\let\clearpage\relax \chapter*{Постановка задачи}}

В задаче требовалось реализовать параллельную сортировку Бэтчера для структур,
представляющих точки на регулярной сетке (\texttt{Point}), вдоль одной из
координат ($x$ или $y$).

Структура \texttt{Point} имеет следующий вид:
\begin{verbatim}
    Point {
        float coord[2];
        int index;
    };
\end{verbatim}

Пусть сетка размерности $n_1 * n_2$ представляется массивом таких структур
\texttt{P[n1 * n2]}, а для инициализации координат точек используются функции
\texttt{x(i,~j)} и \texttt{y(i,~j)}.
Тогда \texttt{P[i*n2+j].coord[0] = x(i,~j)},
\texttt{P[i*n2+j].coord[1] = y(i,~j)}, \texttt{P[i*n2+j].index = i*n2+j}, где $i = \overline{0, n_1-1},
j = \overline{0,n_2-1}$.

На каждом процессе должно обрабатываться одинаковое количество элементов.
Каждый процесс выполняет упорядочивание элементов независимо от других.
Слияние каждого отсортированного массива должно происходить в соответствии с
расписанием, задаваемым сетью сортировки Бэтчера.

После окончания работы программы на каждом процессе должно находиться одинаковое
количество элементов структуры \texttt{Point}. Каждый элемент структуры
\texttt{Point} одного процесса должен находиться левее по координате
по сравнению с элементом структуры \texttt{Point} любого другого процесса с
б\'{о}льшим рангом.

\chapter*{Метод решения}

\section*{Распределение элементов по процессорам}

Одним из требований к программе является одинаковое количество элементов
на процессорах. Чтобы выполнить это требование, при необходимости
исходный массив дополняется фиктивными элементами типа \texttt{Point} с
отрицательным значением индекса. Распределение фиктивных элементов происходит
при инициализации массивов на каждом из процессоров.

Наличие фиктивного элемента на процессоре зависит от его номера (\texttt{rank}).
Количество процессоров, на которых нет фиктивных элементов, равно остатку от
деления количества элементов в исходном массиве на количество процессоров.
Если номер процессора меньше этого значения, то инициализируются все элементы
соответствующего фрагмента массива. В противном случае, последний элемент
массива считается фиктивным, и его поле \texttt{index} инициализируется
отрицательным значением. Такие элементы будут игнорироваться при выводе
конечного результата.

Распределение элементов реализовано в функции \texttt{init\_points} в
файле \texttt{bsort.cpp}:

\begin{verbatim}
Point* init_points(int n, int ny, int procs,
                   int proc_elems, int rank)
{
    Point *res = new Point[proc_elems];
    int tmp = n/procs;
    int not_fake = n % procs;
    int real_elems = rank < not_fake ? tmp + 1 : tmp;
    int delta;
    if (rank < not_fake)
        delta = rank*proc_elems;
    else
        delta = not_fake*proc_elems + tmp*(rank - not_fake);
    for (int k = 0; k < real_elems; k++) {
        int i = (k + delta) / ny, j = (k + delta) % ny;
        Point p;
        p.coord[0] = x(i, j);
        p.coord[1] = y(i, j);
        p.index = i*ny + j;
        res[k] = p;
    }
    return res;
}
\end{verbatim}

\section*{Сортировка и слияние фрагментов}

Локальная сортировка осуществляется вдоль оси $Ox$ при помощи алгоритма \texttt{dhsort},
который является оптимизированной версией нерекурсивного алгоритма сортировки
слиянием (\texttt{dsort}), в котором предварительно короткие фрагменты
массива сортируются алгоритмом пирамидальной сортировки (\texttt{hsort}).
Сортировка коротких частей массива происходит в отдельных потоках.
Для реализации многопоточности используются стандартные функции
POSIX Threads.

Сеть слияния представляет собой сеть сортировки Бэтчера для массива, имеющего
длину, равную количеству процессоров. Каждый элемент массива равен своему индексу,
то есть номеру процессора.

Для построения этой сети сортировки используются две рекурсивные функции,
\texttt{sort} и \texttt{join}.

В функции \texttt{sort} массив рекурсивно разбивается на два подмассива,
которые рекурсивно сортируются и передаются в функцию \texttt{join}, которая
производит их слияние. База рекурсии~--- массив из одного элемента,
который уже очевидным образом отсортирован.

Элементы с чётными и нечётными номерами объединяются отдельно рекурсивными
вызовами функции \texttt{join}. Если в подмассивах по одному элементу,
эти элементы соединяются компаратором. Если суммарное число элементов в
подмассивах больше двух, то после объединения чётных и нечётных элементов,
пары соседних элементов обрабатываются с помощью заключительной
группы компараторов. Массивы чётных и нечётных индексов хранятся в явном виде
и строятся при каждом рекурсивном вызове функции \texttt{join}.

Таким образом, после обработки всего массива индексов процессоров имеем
массив компараторов, который определяет, какие процессоры должны взаимодействовать
при слиянии отсортированных фрагментов. Каждый компаратор представляет собой
пару целых чисел~--- номеров процессоров.

Компараторы обрабатываются следующим образом: два процессора с соответствующими
номерами отправляют друг другу свои отсортированные фрагменты массивов (их
длина одинакова), а затем перераспределяют элементы так, что на первом из
них содержатся элементы с меньшими значениями, а на втором~--- с большими.

Для обмена данными между процессорами используются функции \texttt{MPI\_Send}
и \texttt{MPI\_Recv}.

После того, как вся сеть компараторов будет обработана, на процессорах
будут находится фрагменты упорядоченного массива, причём будет выполнено
свойство: $a_{ir} \leq a_{jr}$ при $i < j$, и $a_{kp} \leq a_{lt}$, при
$p < t$ и любых допустимых значениях $k, l$, где $i, j, k, l$~--- номера
элементов в массиве на процессоре, а $r, p, t$~--- номера процессоров.

Реализация приведена в приложении \ref{app:par}.

При компиляции программы использовался флаг оптимизации \texttt{-O3}.
Строка компиляции:
\verb|mpicxx -g -Wall -O3 bsort.cpp -lpthread -o bsort|.

\chapter*{Используемая вычислительная система}

Вычисления проводились на системе IBM Blue Gene/P.
Характеристики системы представлены на сайте \verb|http://hpc.cmc.msu.ru/bgp|.

Система состоит из двух стоек, включающих 8192 процессорных ядер
(2*1024 четырехъядерных вычислительных узлов), с пиковой производительностью
27,9 терафлопс (27,8528 триллионов операций с плавающей точкой в секунду).

Характеристики вычислительного узла:

\begin{itemize}
    \item четыре микропроцессорных ядра;
    \item пиковая производительность: 13,6 ГФлопс;
    \item пропускная способность памяти: 13,6 ГБ/с;
    \item 2 ГБ общей памяти;
    \item 2 * 4 МБ кэш-памяти 2-го уровня;
    \item легковесное ядро, представляющее собой Linux-подобную операционную систему,
        поддерживающую значительное подмножество Linux-совместимых системных вызовов;
    \item асинхронные операции межпроцессорных обменов (выполняются параллельно с вычислениями);
    \item операции ввода-вывода перенаправляются I/O-картам через сеть коллективных операций;
    \item MPI-операции типа «точка-точка» осуществляются через сеть трехмерного тора.
        \begin{itemize}
            \item Вычислительный узел имеет двунаправленные связи с шестью соседями.
            \item Пропускная способность каждого соединения — 425 МБ/с (5,1 ГБ/с для всех 12
        каналов).
            \item Латентность (ближайший сосед):
                \begin{itemize}
                    \item 32-байтный пакет~--- 0,1 мкс.
                    \item 256-байтный пакет~--- 0,8 мкс.
                \end{itemize}
        \end{itemize}
\end{itemize}


\chapter*{Анализ полученных результатов}

\section*{Последовательный алгоритм сортировки}
\label{seq_alg}
Для выбора последовательного алгоритма, работающего за наименьшее время,
было протестировано несколько алгоритмов сортировки, а именно:
стандартная функция \texttt{qsort}, сортировка слиянием \texttt{dsort},
пирамидальная сортировка \texttt{hsort}, комбинация алгоритмов \texttt{dsort}
и \texttt{hsort}. Поскольку в качестве времени работы последовательного
алгоритма должно использоваться время его работы на одном процессоре на одном ядре,
в функции \texttt{dhsort} короткие фрагменты массива последовательно
(без разделения на потоки) сортируются функцией \texttt{hsort}, после

Измерение времени работы производилось на сетках размера 2000*2000,
4000*4000, 5000*5000, 8000*8000 и 10000*10000. В таблице~\ref{seq_time_tab}
и на рисунке~\ref{seq_fig} приведены результаты
измерений, а также значение $K = \frac{T_1 * 10^9}{[ n * log(n)]}$, где
$T_1$~--- время работы последовательного алгоритма на $n$ элементах.
Прочерки в таблице означают, что программа завершилась с ошибкой из-за
нехватки памяти.

\begin{table}[hp]
\centering
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}\hline
    n       & $T_1(q)$ & $T_1(d)$ & $T_1(h)$ & $T_1(dh)$ & $K_q$ & $K_d$ & $K_h$ & $K_{dh}$  \\ \hline
2000*2000   & 10.39    &  4.67    & 11.34    & 5.5       & 118.43& 53.23 & 129.26& 62.69   \\ \hline
4000*4000   & 45.32    &  20.29   & 53.68    & 23.38     & 118.35& 52.98 & 140.19& 61.05   \\ \hline
5000*5000   & 73.48    &  32.95   & 88.15    & 37.85     & 119.59& 53.63 & 143.47& 61.60   \\ \hline
8000*8000   & 103.85   &  90.79   & 247.69   & 98.34     & 62.57 & 54.70 & 149.24& 59.25   \\ \hline
10000*10000 & 163.83   &  ---     & 403.73   & ---       & 61.64 & ---   & 151.91& ---     \\ \hline
\end{tabular}
\caption{Время работы и значения $K$}
\label{seq_time_tab}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{seq}
    \caption{Значение константы $K = \frac{T_1 * 10^9}{[ n * log(n)]}$ для различных последовательных алгоритмов сортировки}
    \label{seq_fig}
\end{figure}

Таким образом, наилучшее время показывает алгоритм сортировки слиянием
\texttt{dsort}. Среднее значение $K_d = 53.63$. При вычислении эффективности распараллеливания в качестве
времени $T_1$ используется время работы алгоритма \texttt{dsort}
на соответствующей сетке.

Реализация указанных функций представлена в приложении \ref{app:seq}.

\section*{Эффективность распараллеливания}

Тестирование параллельной программы проводилось на сетках размером
2000*2000, 4000*4000, 5000*5000 и 8000*8000 элементов, на 8, 16, 32, 128
и 256 процессорах.

Время работы программы приведено в таблице \ref{par_time_tab} и на
рисунке \ref{par_time_fig}.

\begin{table}[h]
\centering
\begin{tabular}{|r|r|r|r|r|}\hline
\backslashbox{Процессы}{Сетка}
       & 2000*2000 & 4000*4000 & 5000*5000 & 8000*8000 \\ \hline
1      & 4.67      & 20.29     &  32.95    & 90.79     \\ \hline
8      & 0.6154    & 2.631084  &  4.159453 & 11.445924 \\ \hline
16     & 0.447151  & 1.597961  &  2.381565 & 6.103069  \\ \hline
32     & 0.300547  & 1.214     &  1.764936 & 4.379375  \\ \hline
64     & 0.197966  & 0.802711  &  1.269715 & 3.047342  \\ \hline
128    & 0.129848  & 0.523304  &  0.823766 & 2.094921  \\ \hline
256    & 0.096124  & 0.374291  &  0.588822 & 1.514643  \\ \hline
\end{tabular}
\caption{Время работы, сек.}
\label{par_time_tab}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{time}
    \caption{Зависимость времени работы параллельного алгоритма от числа процессоров}
    \label{par_time_fig}
\end{figure}

Эффективность рассчитывалась по формуле: $E = \frac{T_1}{p*T_p}$, где
$T_1$~--- время работы наилучшего последовательного алгоритма,
$T_p$~--- время работы параллельного алгоритма на $p$ процессорах.
Результаты приведены в таблице \ref{eff_tab} и на рисунке \ref{eff_fig}.

\begin{table}[h]
\centering
\begin{tabular}{|r|r|r|r|r|}\hline
\backslashbox{Процессы}{Сетка}
       & 2000*2000 & 4000*4000 & 5000*5000 & 8000*8000 \\ \hline
8      & 0.94857   &  0.963956 & 0.990214  &  0.99151  \\ \hline
16     & 0.652743  &  0.793589 & 0.864715  &  0.929757 \\ \hline
32     & 0.485572  &  0.522292 & 0.583413  &  0.647852 \\ \hline
64     & 0.368592  &  0.394950 & 0.405479  &  0.465518 \\ \hline
128    & 0.280977  &  0.302913 & 0.312493  &  0.338579 \\ \hline
256    & 0.189777  &  0.211754 & 0.21859   &  0.234146 \\ \hline
\end{tabular}
\caption{Эффективность распараллеливания}
\label{eff_tab}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{eff}
    \caption{Зависимость эффективности распараллеливания от числа процессоров}
    \label{eff_fig}
\end{figure}

Для наглядности на графике на рисунке \ref{eff_proc} показана зависимость
эффективности распараллеливания от количества элементов.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{eff_proc}
    \caption{Зависимость эффективности распараллеливания от количества элементов}
    \label{eff_proc}
\end{figure}

Максимально возможную эффективность распараллеливания на $p$ процессорах
можно рассчитать по формуле $E_{max}(p) = \frac{M^*(p)}{M(p)}$,
где $M^*$~--- число компараторов в сети слияния Бэтчера,
а $M$~--- максимально возможное число компараторов, равное $tacts*\frac{p}{2}$.
В таблице \ref{max_eff_tab} приведены значения этой величины для различного
числа процессоров.

\begin{table}[h]
\centering
\begin{tabular}{|r|r|r|r|r|}\hline
$p$ & $tacts$ & $M$ & $M^*$ & $E_{max}$ \\ \hline
8   & 6       & 24  & 19    & 0.79      \\ \hline
16  & 10      & 80  & 63    & 0.78      \\ \hline
32  & 15      & 240 & 191   & 0.8       \\ \hline
64  & 21      & 672 & 543   & 0.8       \\ \hline
128 & 28      & 1792& 1471  & 0.82      \\ \hline
256 & 36      & 4608& 3839  & 0.83      \\ \hline
\end{tabular}
\caption{Максимально возможная эффективность распараллеливания}
\label{max_eff_tab}
\end{table}

Как видно из графиков, эффективность
распараллеливания уменьшается
при увеличении числа процессоров из-за роста времени передачи данных
(массивов структур типа \texttt{Point}). Возможными причинами, из-за которых
наблюдается такой эффект, могут служить деградация сети и отсутствие учёта
топологии сети при обмене данными. С другой стороны, наблюдается рост
эффективности при увеличении количества элементов в сортируемом массиве.
На 128 вычислительных ядрах (32 процессора по 4 ядра)
эффективность программы составляет 60-80\% от максимально возможной. При
использовании 16 процессоров (64 вычислительных ядра) значение эффективности
не опускается ниже 83\% от максимально достижимой.

\newpage
\appendix
\chapter{Последовательные алгоритмы сортировки} \label{app:seq}
\begin{verbatim}
    #define MAX_HSORT_ELEMS 5000

    void heapify(Point *a, int i, int n)
    {
        int imax, l, r;
        Point tmp;
        while (i < n) {
            imax = i;
            l = 2*i + 1;
            r = l + 1;
            if (l < n && (compare_points(&a[l], &a[imax]) > 0))
                imax = l;
            if (r < n && (compare_points(&a[r], &a[imax]) > 0))
                imax = r;
            if (imax == i)
                return;
            tmp = a[i];
            a[i] = a[imax];
            a[imax] = tmp;
            i = imax;
        }
    }

    void make_heap(Point *a, int n)
    {
        for (int i = n/2 - 1; i >= 0; i--)
            heapify(a, i, n);
    }

    void hsort(Point *a, int n)
    {
        make_heap(a, n);
        Point tmp;
        for (int i = n - 1; i > 0; i--) {
            tmp = a[0];
            a[0] = a[i];
            a[i] = tmp;
            heapify(a, 0 ,i);
        }
    }

    void dsort (Point *array, int n, int sorted)
    {
        Point *a = array;
        Point *b = new Point[n];
        Point *c;

        for (int i = sorted; i < n ; i *= 2) {
            for (int j = 0; j < n; j = j + 2*i) {
                int r = j + i;

                int n1 = (i < n - j) ? i : n - j;
                int n2 = (i < n - r) ? i : n - r;
                n1 = (n1 < 0) ? 0 : n1;
                n2 = (n2 < 0) ? 0 : n2;

                for (int ia = 0, ib = 0, k = 0; k < n1 + n2; k++) {
                    if (ia >= n1)
                        b[j+k] = a[r+ib++];
                    else if (ib >= n2)
                        b[j+k]=a[j+ia++];
                    else if (compare_points(&a[j+ia], &a[r+ib]) < 0)
                        b[j+k]=a[j+ia++];
                    else
                        b[j+k]=a[r+ib++];
                }
            }
            c = a;
            a = b;
            b = c;
        }

        c = a;
        a = b;
        b = c;
        if (b != array) {
            memcpy(array, b, n*sizeof(*array));
            delete [] b;
        } else {
            delete [] a;
        }
    }

    void dhsort(Point *a, int n)
    {
        int nthreads = ceil(n / (double)MAX_HSORT_ELEMS);
        nthreads = nthreads > 4 ? nthreads : 4;
        int tmp = ceil(n / (double)nthreads);
        int elems;
        int offset = 0;
        for (int th = 0; th < nthreads; th++) {
            if (n - offset >= tmp)
                elems = tmp;
            else
                elems = (n - offset > 0) ? n - offset : 0;
            hsort(a + offset, elems);
            offset += elems;
        }
        dsort(a, n, ceil(n / (double)nthreads));
        return;
    }
\end{verbatim}

\chapter{Параллельная сортировка Бэтчера} \label{app:par}
\begin{verbatim}

// File bsort.cpp
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include <time.h>
#include <string.h>
#include <cmath>
#include <vector>
#include <pthread.h>
#include <cstddef>


#define MIN_THREADS_NUM 4
#define MAX_HSORT_ELEMS 100000

struct Point {
    float coord[2];
    int index;
};

float x(int i, int j)
{
    return (float)rand()/(float)(RAND_MAX/(i*j+1));
}

float y(int i, int j)
{
    return (float)rand()/(float)(RAND_MAX/(i*j+1));
}

Point* init_points(int n, int ny, int procs, int proc_elems, int rank)
{
    Point *res = new Point[proc_elems];
    int tmp = n/procs;
    int not_fake = n % procs;
    int real_elems = rank < not_fake ? tmp + 1 : tmp;
    int delta;
    if (rank < not_fake)
        delta = rank*proc_elems;
    else
        delta = not_fake*proc_elems + tmp*(rank - not_fake);
    for (int k = 0; k < real_elems; k++) {
        int i = (k + delta) / ny, j = (k + delta) % ny;
        Point p;
        p.coord[0] = x(i, j);
        p.coord[1] = y(i, j);
        p.index = i*ny + j;
        res[k] = p;
    }
    return res;
}

MPI_Datatype pointType()
{
    MPI_Datatype point;
    MPI_Datatype types[2] = { MPI_FLOAT, MPI_INT };
    int blocks[2] = { 2, 1 };
    MPI_Aint disps[2] = { offsetof(Point, coord),
                          offsetof(Point, index) };
    MPI_Type_create_struct(2, blocks, disps, types, &point);
    MPI_Type_commit(&point);
    return point;
}

typedef std::pair<int, int> comparator;

void swap(comparator cmp, std::vector<int> &v)
{
    int fst = cmp.first;
    int snd = cmp.second;
    if (v[fst] > v[snd]) {
        int tmp = v[fst];
        v[fst] = v[snd];
        v[snd] = tmp;
    }
}

void print_vector(std::vector<int> &v, int n)
{
    for (int i = 0; i < n; i++)
        printf("%d ", v[i]);
    putchar('\n');
}

void print_comparators(std::vector<comparator> &cmp)
{
    std::vector<comparator>::iterator it;
    for (it = cmp.begin(); it != cmp.end(); it++)
        printf("%d %d\n", it->first, it->second);
    printf("%lu\n", cmp.size());
}

void print_points(Point *p, int n, int rank, const char *comment)
{
    for (int i = 0; i < n; i++) {
        Point point = p[i];
        if (point.index >= 0)
            printf("%d %d: %f %s\n", rank, point.index,
                   point.coord[0], comment);
    }
}

bool check_args(int argc, char **argv, int &nx, int &ny)
{
    if (argc < 3) {
        printf("Wrong arguments. Usage: bsort nx ny\n");
        return false;
    }
    int check = sscanf(argv[1], "%d", &nx);
    if (!check) {
        printf("nx must be int: %s\n", argv[1]);
        return false;
    }
    check = sscanf(argv[2], "%d", &ny);
    if (!check) {
        printf("ny must be int: %s\n", argv[2]);
        return false;
    }
    if (!((nx >= 1) && (ny >= 1))) {
        printf("Wrong n1 or n2\n");
        return false;
    }
    return true;
}

int count_tacts(int n, std::vector<comparator> &cmp)
{
    std::vector<int> v(n);
    std::vector<comparator>::iterator it;
    int max;
    for (it = cmp.begin(); it != cmp.end(); it++) {
        int fst = it->first;
        int snd = it->second;
        max = v[fst] > v[snd] ? v[fst] : v[snd];
        v[fst] = max + 1;
        v[snd] = max + 1;
    }
    max = 0;
    for (int i = 0; i < n; i++)
        if (v[i] > max)
            max = v[i];
    return max;
}

void swap_ptr(void *ptr1_ptr, void *ptr2_ptr)
{
    void **ptr1 = (void **)ptr1_ptr;
    void **ptr2 = (void **)ptr2_ptr;

    void *tmp = *ptr1;
    *ptr1 = *ptr2;
    *ptr2 = tmp;
}

void write_output(Point *a, int n, char *file,
                  int nx, int ny, int rank)
{
    float *a_out = new float[n];
    int out_cnt = 0;
    for (int i = 0; i < n; i++) {
        if (a[i].index != -1)
            a_out[out_cnt++] = a[i].coord[0];
    }

    MPI_Status status;
    MPI_File output;
    int res = MPI_File_open(MPI_COMM_WORLD, file,
                            MPI_MODE_CREATE | MPI_MODE_WRONLY,
                            MPI_INFO_NULL, &output);
    if (res != MPI_SUCCESS) {
        if (rank == 0) {
            printf("Cannot open %s\n", file);
        }
        MPI_Finalize();
        exit(1);
    }
    MPI_File_set_size(output, 0);
    MPI_File_write_ordered(output, a_out, out_cnt,
                           MPI_FLOAT, &status);
    MPI_File_close(&output);
    delete [] a_out;

    return;
}
inline int compare_points(const void *a, const void *b)
{
  Point *aptr = (Point * const)a;
  Point *bptr = (Point * const)b;
  float ax = aptr->coord[0];
  float bx = bptr->coord[0];

  if (ax == bx)
      return 0;
  else if (ax > bx)
      return 1;
  return -1;
}

struct PthreadArgs {
    pthread_t tid;
    Point *a;
    int n;
};

void heapify(Point *a, int i, int n)
{
    int imax, l, r;
    Point tmp;
    while (i < n) {
        imax = i;
        l = 2*i + 1;
        r = l + 1;
        if (l < n && (compare_points(&a[l], &a[imax]) > 0))
            imax = l;
        if (r < n && (compare_points(&a[r], &a[imax]) > 0))
            imax = r;
        if (imax == i)
            return;
        tmp = a[i];
        a[i] = a[imax];
        a[imax] = tmp;
        i = imax;
    }
}

void make_heap(Point *a, int n)
{
    for (int i = n/2 - 1; i >= 0; i--)
        heapify(a, i, n);
}

void hsort(Point *a, int n)
{
    make_heap(a, n);
    Point tmp;
    for (int i = n - 1; i > 0; i--) {
        tmp = a[0];
        a[0] = a[i];
        a[i] = tmp;
        heapify(a, 0 ,i);
    }
}

void* hsort_start(void *arg)
{
    PthreadArgs *args = (PthreadArgs*)arg;
    hsort(args->a, args->n);
    return NULL;
}


void hsort_threads(Point *a, int n, int nthreads)
{
    PthreadArgs *pthread_args = new PthreadArgs[nthreads];
    int tmp = ceil(n / (double)nthreads);
    int elems;
    int offset = 0;
    for (int th = 0; th < nthreads; th++) {
        if (n - offset >= tmp)
            elems = tmp;
        else
            elems = (n - offset > 0) ? n - offset : 0;

        pthread_args[th].a = a + offset;
        pthread_args[th].n = elems;
        pthread_create(&pthread_args[th].tid, NULL, hsort_start,
                       &pthread_args[th]);
        offset += elems;
    }
    for (int th = 0; th < nthreads; th++)
        pthread_join(pthread_args[th].tid, NULL);
    delete [] pthread_args;
}

void dsort (Point *array, int n, int sorted)
{
    Point *a = array;
    Point *b = new Point[n];
    Point *c;

    for (int i = sorted; i < n ; i *= 2) {
        for (int j = 0; j < n; j = j + 2*i) {
            int r = j + i;

            int n1 = (i < n - j) ? i : n - j;
            int n2 = (i < n - r) ? i : n - r;
            n1 = (n1 < 0) ? 0 : n1;
            n2 = (n2 < 0) ? 0 : n2;

            for (int ia = 0, ib = 0, k = 0; k < n1 + n2; k++) {
                if (ia >= n1)
                    b[j+k] = a[r+ib++];
                else if (ib >= n2)
                    b[j+k]=a[j+ia++];
                else if (compare_points(&a[j+ia], &a[r+ib]) < 0)
                    b[j+k]=a[j+ia++];
                else
                    b[j+k]=a[r+ib++];
            }
        }
        c = a;
        a = b;
        b = c;
    }

    c = a;
    a = b;
    b = c;
    if (b != array) {
        memcpy(array, b, n*sizeof(*array));
        delete [] b;
    } else {
        delete [] a;
    }
}

void dhsort(Point *a, int n)
{
    int nthreads = ceil(n / (double)MAX_HSORT_ELEMS);
    nthreads = nthreads > MIN_THREADS_NUM ?
               nthreads : MIN_THREADS_NUM;
    hsort_threads(a, n, nthreads);
    dsort(a, n, ceil(n / (double)nthreads));

    return;
}

void join(std::vector<int> idx_up, int n0, std::vector<int> idx_down,
          int n1, std::vector<comparator> &cmp)
{
    int n = n0 + n1;
    if (n == 1)
        return;

    if (n0 == 1 && n1 == 1) {
        cmp.push_back(comparator(idx_up[0], idx_down[0]));
        return;
    }

    int n0_even = n0/2;
    int n0_odd = n0 - n0_even;
    std::vector<int> idx_up_even(n0_even);
    std::vector<int> idx_up_odd(n0_odd);

    int n1_even = n1/2;
    int n1_odd = n1 - n1_even;
    std::vector<int> idx_down_even(n1_even);
    std::vector<int> idx_down_odd(n1_odd);

    std::vector<int> idx_result;

    int i, i0 = 0, i1 = 0;
    for (i = 0; i < n0; i++)
        if (i%2) {
            idx_up_even[i0] = idx_up[i];
            i0++;
        } else {
            idx_up_odd[i1] = idx_up[i];
            i1++;
        }
    i0 = i1 = 0;
    for (i = 0; i < n1; i++)
        if (i%2) {
            idx_down_even[i0] = idx_down[i];
            i0++;
        } else {
            idx_down_odd[i1] = idx_down[i];
            i1++;
        }

    join(idx_up_odd, n0_odd, idx_down_odd, n1_odd, cmp);
    join(idx_up_even, n0_even, idx_down_even, n1_even, cmp);

    for (i = 0; i < n0; i++)
        idx_result.push_back(idx_up[i]);
    for (i = 0; i < n1; i++)
        idx_result.push_back(idx_down[i]);

    for (int i = 1; i < n - 1; i += 2)
        cmp.push_back(comparator(idx_result[i], idx_result[i + 1]));
}

void sort(std::vector<int>idx, int n, std::vector<comparator> &cmp)
{
    if (n == 1) {
        return;
    }

    int n0 = n/2;
    int n1 = n - n0;

    std::vector<int> idx_up;
    std::vector<int> idx_down;

    int i;
    for (i = 0; i < n0; i++)
        idx_up.push_back(idx[i]);
    for (i = n0; i < n; i++)
        idx_down.push_back(idx[i]);

    sort(idx_up, n0, cmp);
    sort(idx_down, n1, cmp);
    join(idx_up, n0, idx_down, n1, cmp);
}

void make_comparators(int procs, std::vector<comparator> &cmp)
{
    std::vector<int> idx;
    for (int i = 0; i < procs; i++)
        idx.push_back(i);
    sort(idx, procs, cmp);
    return;
}

int main(int argc, char **argv)
{
    int nx, ny;

    // Parsing command line arguments
    if (!check_args(argc, argv, nx, ny))
        return 1;

    MPI_Init(&argc, &argv);

    int rank, procs;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &procs);

    // Comparators network
    std::vector<comparator> cmp;
    make_comparators(procs, cmp);

    // Calculating elems per processor
    int n = nx*ny;
    int fake = n % procs ? (procs - n % procs) : 0;
    int elems = n + fake;
    int proc_elems = elems / procs;

    // Initializing points
    srand(time(NULL) + rank);
    Point *proc_points =
        init_points(n, ny, procs, proc_elems, rank);

    // Sorting
    double start_time = MPI_Wtime();
    dhsort(proc_points, proc_elems);

    // Exchanging elements
    Point *tmp_points = new Point[proc_elems];
    Point *other_points = new Point[proc_elems];
    MPI_Status status;
    MPI_Datatype MPI_POINT = pointType();
    std::vector<comparator>::iterator it;
    for (it = cmp.begin(); it != cmp.end(); it++) {
        if (rank == it->first) {
            MPI_Send(proc_points, proc_elems, MPI_POINT,
                     it->second, 0, MPI_COMM_WORLD);
            MPI_Recv(other_points, proc_elems, MPI_POINT,
                     it->second, 0, MPI_COMM_WORLD, &status);
            int idx = 0;
            int other_idx = 0;
            for (int tmp_idx = 0; tmp_idx < proc_elems; tmp_idx++) {
                Point my = proc_points[idx];
                Point other = other_points[other_idx];
                if (my.coord[0] < other.coord[0]) {
                    tmp_points[tmp_idx] = my;
                    idx++;
                } else {
                    tmp_points[tmp_idx] = other;
                    other_idx++;
                }
            }
            swap_ptr(&proc_points, &tmp_points);
        }

        if (rank == it->second) {
            MPI_Recv(other_points, proc_elems, MPI_POINT,
                     it->first, 0, MPI_COMM_WORLD, &status);
            MPI_Send(proc_points, proc_elems, MPI_POINT,
                     it->first, 0, MPI_COMM_WORLD);
            int idx = proc_elems - 1;
            int other_idx = proc_elems - 1;
            for (int tmp_idx = idx; tmp_idx >= 0; tmp_idx--) {
                Point my = proc_points[idx];
                Point other = other_points[other_idx];
                if (my.coord[0] > other.coord[0]) {
                    tmp_points[tmp_idx] = my;
                    idx--;
                } else {
                    tmp_points[tmp_idx] = other;
                    other_idx--;
                }
            }
            swap_ptr(&proc_points, &tmp_points);
        }
    }
    double end_time = MPI_Wtime();
    double time = end_time - start_time;
    double sort_time = 0;
    MPI_Reduce(&time, &sort_time, 1,
               MPI_FLOAT, MPI_MAX, 0, MPI_COMM_WORLD);

    if (!rank) {
        printf("Elems: %d\nProcs: %d\n", n, procs);
        printf("Sort time: %f sec.\n", sort_time);
    }

    if (argc > 3) {
        write_output(proc_points, proc_elems, argv[3], nx, ny, rank);
    }

    delete [] proc_points;
    MPI_Finalize();
    return 0;
}

\end{verbatim}

\end{document}


