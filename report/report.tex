\documentclass[oneside,final,14pt]{extreport}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{vmargin}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{diagbox}
\newcommand*{\No}{\textnumero}
\setpapersize{A4}
\setmarginsrb{3cm}{2cm}{1cm}{2cm}{0pt}{0mm}{0pt}{13mm}
\usepackage{indentfirst}
\sloppy
\clubpenalty = 10000
\widowpenalty = 10000

\begin{document}
\vspace{0.4cm}
\begin{large}
\noindent
\textit{Галкина Екатерина Владимировна\\
Группа 524\\
2016 год}
\end{large}

\vspace{0.4cm}

{\let\clearpage\relax \chapter*{Постановка задачи}}

В задаче требовалось реализовать параллельную сортировку Бэтчера для структур,
представляющих точки на регулярной сетке (\texttt{Point}), вдоль одной из
координат ($x$ или $y$).

Структура \texttt{Point} имеет следующий вид:
\begin{verbatim}
    Point {
        float coord[2];
        int index;
    };
\end{verbatim}

Пусть сетка размерности $n_1 * n_2$ представляется массивом таких структур
\texttt{P[n1 * n2]}, а для инициализации координат точек используются функции
\texttt{x(i,~j)} и \texttt{y(i,~j)}.
Тогда \texttt{P[i*n2+j].coord[0] = x(i,~j)},
\texttt{P[i*n2+j].coord[1] = y(i,~j)}, \texttt{P[i*n2+j].index = i*n2+j}, где $i = \overline{0, n_1-1},
j = \overline{0,n_2-1}$.

На каждом процессе должно обрабатываться одинаковое количество элементов.
Каждый процесс выполняет упорядочивание элементов независимо от других.
Слияние каждого отсортированного массива должно происходить в соответствии с
расписанием, задаваемым сетью сортировки Бэтчера.

После окончания работы программы на каждом процессе должно находиться одинаковое
количество элементов структуры \texttt{Point}. Каждый элемент структуры
\texttt{Point} одного процесса должен находиться левее по координате
по сравнению с элементом структуры \texttt{Point} любого другого процесса с
б\'{о}льшим рангом.

\chapter*{Метод решения}

\section*{Распределение элементов по процессам}

Одним из требований к программе является одинаковое количество элементов
на процессах. Чтобы выполнить это требование, при необходимости
исходный массив дополняется фиктивными элементами типа \texttt{Point} с
отрицательным значением индекса. Распределение фиктивных элементов происходит
при инициализации массивов на каждом из процессов.

Наличие фиктивного элемента на процессе зависит от его номера (\texttt{rank}).
Количество процессов, на которых нет фиктивных элементов, равно остатку от
деления количества элементов в исходном массиве на количество процессов.
Если номер процесса меньше этого значения, то инициализируются все элементы
соответствующего фрагмента массива. В противном случае, последний элемент
массива считается фиктивным, и его поле \texttt{index} инициализируется
отрицательным значением. Такие элементы будут игнорироваться при выводе
конечного результата.

Распределение элементов реализовано в функции \texttt{init\_points} в
файле \texttt{point.cpp}:

\begin{verbatim}
Point* init_points(int n, int ny, int procs,
                   int proc_elems, int rank)
{
    Point *res = new Point[proc_elems];
    int tmp = n/procs;
    int not_fake = n % procs;
    int real_elems = rank < not_fake ? tmp + 1 : tmp;
    int delta;
    if (rank < not_fake)
        delta = rank*proc_elems;
    else
        delta = not_fake*proc_elems + tmp*(rank - not_fake);
    for (int k = 0; k < real_elems; k++) {
        int i = (k + delta) / ny, j = (k + delta) % ny;
        Point p;
        p.coord[0] = x(i, j);
        p.coord[1] = y(i, j);
        p.index = i*ny + j;
        res[k] = p;
    }
    return res;
}
\end{verbatim}

\section*{Сортировка и слияние фрагментов}

Локальная сортировка осуществляется вдоль оси $Ox$ при помощи алгоритма \texttt{dhsort},
который является оптимизировнной версией нерекурсивного алгоритма сортировки
слиянием (\texttt{dsort}), в котором предварительно короткие фрагменты
массива сортируются алгоритмом пирамидальной сортировки (\texttt{hsort}).
Сортировка коротких частей массива происходит в отдельных потоках.
Для реализации многопоточности используются стандартные функции
POSIX Threads.

Сеть слияния представляет собой сеть сортировки Бэтчера для массива, имеющего
длину, равную количеству процессов. Каждый элемент массива равен своему индексу,
то есть номеру процесса.

Для построения этой сети сортировки используются две рекурсивные функции,
\texttt{sort} и \texttt{join}.

В функции \texttt{sort} массив рекурсивно разбивается на два подмассива,
которые рекурсивно сортируются и передаются в функцию \texttt{join}, которая
производит их слияние. База рекурсии~--- массив из одного элемента,
который уже очевидным образом отсортирован.

Элементы с чётными и нечётными номерами объединяются отдельно рекурсивными
вызовами функции \texttt{join}. Если в подмассивах по одному элементу,
эти элементы соединяются компаратором. Если суммарное число элементов в
подмассивах больше двух, то после объединения чётных и нечётных элементов,
пары соседних элементов обрабатываются с помощью заключительной
группы компараторов. Массивы чётных и нечётных индексов хранятся в явном виде
и строятся при каждом рекурсивном вызове функции \texttt{join}.

Таким образом, после обработки всего массива индексов процессов имеем
массив компараторов, который определяет, какие процессы должны взаимодействовать
при слиянии отсортированных фрагментов. Каждый компаратор представляет собой
пару целых чисел~--- номеров процессов.

Компараторы обрабатываются следующим образом: два процесса с соответствующими
номерами отправляют друг другу свои отсортированные фрагменты массивов (их
длина одинакова), а затем перераспределяют элементы так, что на первом из
них содержатся элементы с меньшими значениями, а на втором~--- с большими.

Для обмена данными между процессами используются функции \texttt{MPI\_Send}
и \texttt{MPI\_Recv}.

После того, как вся сеть компараторов будет обработана, на процессах
будут находится фрагменты упорядоченного массива, причём будет выполнено
свойство: $a_{ir} \leq a_{jr}$ при $i < j$, и $a_{kp} \leq a_{lt}$, при
$p < t$ и любых допустимых значениях $k, l$, где $i, j, k, l$~--- номера
элементов в массиве на процессе, а $r, p, t$~--- номера процессов.

Реализация преведена в приложении \ref{app:par}.

\chapter*{Используемая вычислительная система}

Вычисления проводились на системе IBM Blue Gene/P.
Характеристики системы представлены на сайте \verb|http://hpc.cmc.msu.ru/bgp|.

Система состоит из двух стоек, включающих 8192 процессорных ядер
(2*1024 четырехъядерных вычислительных узлов), с пиковой производительностью
27,9 терафлопс (27,8528 триллионов операций с плавающей точкой в секунду).

Характеристики вычислительного узла:

\begin{itemize}
    \item четыре микропроцессорных ядра;
    \item пиковая производительность: 13,6 ГФлопс;
    \item пропускная способность памяти: 13,6 ГБ/с;
    \item 2 ГБ общей памяти;
    \item 2 * 4 МБ кэш-памяти 2-го уровня;
    \item легковесное ядро, представляющее собой Linux-подобную операционную систему,
        поддерживающую значительное подмножество Linux-совместимых системных вызовов;
    \item асинхронные операции межпроцессорных обменов (выполняются параллельно с вычислениями);
    \item операции ввода-вывода перенаправляются I/O-картам через сеть коллективных операций;
    \item MPI-операции типа «точка-точка» осуществляются через сеть трехмерного тора.
        \begin{itemize}
            \item Вычислительный узел имеет двунаправленные связи с шестью соседями.
            \item Пропускная способность каждого соединения — 425 МБ/с (5,1 ГБ/с для всех 12
        каналов).
            \item Латентность (ближайший сосед):
                \begin{itemize}
                    \item 32-байтный пакет~--- 0,1 мкс.
                    \item 256-байтный пакет~--- 0,8 мкс.
                \end{itemize}
        \end{itemize}
\end{itemize}


\chapter*{Анализ полученных результатов}

\section*{Последовательный алгоритм сортировки}
\label{seq_alg}
Для выбора последовательного алгоритма, работающего за наименьшее время,
было протестировано несколько алгоритмов сортировки, а именно:
стандартная функция \texttt{qsort}, сортировка слиянием \texttt{dsort},
пирамидальная сортировка \texttt{hsort}, комбинация алгоритмов \texttt{dsort}
и \texttt{hsort}. Поскольку в качестве времени работы последовательного
алгоритма должно использоваться время его работы на одном процессе на одном ядре,
в функции \texttt{dhsort} короткие фрагменты массива последовательно
(без разделения на потоки) сортируются функцией \texttt{hsort}, после
чего происходит их слияние.

Измерение времени работы производилось на сетках размера 2000*2000,
4000*4000, 5000*5000, 8000*8000 и 10000*10000. В таблице \ref{seq_time_tab}
приведены результаты
измерений, а также значение $K = \frac{T_1 * 10^9}{[ n * log(n)]}$, где
$T_1$~--- время работы последовательного алгоритма на $n$ элементах.

\begin{table}[hp]
\centering
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}\hline
    n       & $T_1(q)$ & $T_1(d)$ & $T_1(h)$ & $T_1(dh)$ & $K_q$ & $K_d$ & $K_h$ & $K_{dh}$  \\ \hline
2000*2000   & 10.39    & 3.87     & 11.34    & 5.5       & 118.43& 44.11 & 129.26& 62.69   \\ \hline
4000*4000   & 45.32    & 16.69    & 53.68    & 23.38     & 118.35& 43.58 & 140.19& 61.05   \\ \hline
5000*5000   & 73.48    & 26.95    & 88.15    & 37.85     & 119.59& 43.86 & 143.47& 61.60   \\ \hline
8000*8000   & 103.85   & 69.79    & 247.69   & 98.34     & 62.57 & 42.05 & 149.24& 59.25   \\ \hline
10000*10000 & 163.83   & ---      & 403.73   & ---       & 61.64 & ---   & 151.91& ---     \\ \hline
\end{tabular}
\caption{Время работы и значения $K$}
\label{seq_time_tab}
\end{table}

Таким образом, наилучшее время показывает алгоритм сортировки слиянием
\texttt{dhsort}. При вычислении эффективности распараллеливания в качестве
времени $T_1$ используется время работы алгоритма \texttt{dsort}
на соответствующей сетке.

Реализация указанных функций представлена в приложении \ref{app:seq}.

\section*{Эффективность распараллеливания}

Тестирование параллельной программы проводилось на сетках размером
2000*2000, 4000*4000, 5000*5000 и 8000*8000 элементов, на 8, 16, 32, 128
и 256 процессах.

Время работы программы приведено в таблице \ref{par_time_tab}.

\begin{table}[hp]
\centering
\begin{tabular}{|r|r|r|r|r|}\hline
\backslashbox{Процессы}{Сетка}
       & 2000*2000 & 4000*4000 & 5000*5000 & 8000*8000   \\ \hline
1      & 3.87      & 16.69     & 26.95     & 69.79       \\ \hline
8      & 0.6154    & 2.231084  & 3.506908  & 8.895924    \\ \hline
16     & 0.447151  & 1.597961  & 2.381565  & 6.103069    \\ \hline
32     & 0.300547  & 1.214     & 1.764936  & 4.379375    \\ \hline
64     & 0.197966  & 0.802711  & 1.269715  & 3.047342    \\ \hline
128    & 0.129848  & 0.523304  & 0.823766  & 2.094921    \\ \hline
256    & 0.096124  & 0.374291  & 0.588822  & 1.514643    \\ \hline
\end{tabular}
\caption{Время работы, сек.}
\label{par_time_tab}
\end{table}

Эффективность рассчитывалась по формуле: $E = \frac{T_1}{p*T_p}$, где
$T_1$~--- время работы наилучшего последовательного алгоритма,
$T_p$~--- время работы параллельного алгоритма на $p$ процессах.
Результаты приведены в таблице \ref{eff_tab}.

\begin{table}[hp]
\centering
\begin{tabular}{|r|r|r|r|r|}\hline
\backslashbox{Процессы}{Сетка}
       & 2000*2000 & 4000*4000 & 5000*5000 & 8000*8000 \\ \hline
8      & 0.7860741 & 0.9350835 & 0.960604  & 0.9806457 \\ \hline
16     & 0.5409246 & 0.652785  & 0.7072555 & 0.7147018 \\ \hline
32     & 0.4023913 & 0.4296231 & 0.4771773 & 0.4980019 \\ \hline
64     & 0.3054502 & 0.3248756 & 0.3316443 & 0.3578425 \\ \hline
128    & 0.2328444 & 0.249168  & 0.2555906 & 0.2602648 \\ \hline
256    & 0.1572676 & 0.1741834 & 0.1787865 & 0.1799877 \\ \hline
\end{tabular}
\caption{Эффективность распараллеливания}
\label{eff_tab}
\end{table}

Максимально возможную эффективность распараллеливания на $p$ процессах
можно рассчитать по формуле $E_{max}(p) = \frac{M^*(p)}{M(p)}$,
где $M^*$~--- число компараторов в сети слияния Бэтчера,
а $M$~--- максимально возможное число компараторов, равное $tacts*\frac{p}{2}$.
В таблице \ref{max_eff_tab}.

\begin{table}[hp]
\centering
\begin{tabular}{|r|r|r|r|r|}\hline
$p$ & $tacts$ & $M$ & $M^*$ & $E_{max}$ \\ \hline
8   & 6       & 24  & 19    & 0.79      \\ \hline
16  & 10      & 80  & 63    & 0.78      \\ \hline
32  & 15      & 240 & 191   & 0.8       \\ \hline
64  & 21      & 672 & 543   & 0.8       \\ \hline
128 & 28      & 1792& 1471  & 0.82      \\ \hline
256 & 36      & 4608& 3839  & 0.83      \\ \hline
\end{tabular}
\caption{Максимально возможная эффективность распараллеливания}
\label{max_eff_tab}
\end{table}

\newpage
\appendix
\chapter{Последовательные алгоритмы сортировки} \label{app:seq}
\begin{verbatim}
    #define MAX_HSORT_ELEMS 5000

    void heapify(Point* a, int i, int n)
    {
        int imax, l, r;
        Point tmp;
        while (i < n) {
            imax = i;
            l = 2*i + 1;
            r = l + 1;
            if (l < n && (compare_points(&a[l], &a[imax]) > 0))
                imax = l;
            if (r < n && (compare_points(&a[r], &a[imax]) > 0))
                imax = r;
            if (imax == i)
                return;
            tmp = a[i];
            a[i] = a[imax];
            a[imax] = tmp;
            i = imax;
        }
    }

    void make_heap(Point* a, int n)
    {
        for (int i = n/2 - 1; i >= 0; i--)
            heapify(a, i, n);
    }

    void hsort(Point *a, int n)
    {
        make_heap(a, n);
        Point tmp;
        for (int i = n - 1; i > 0; i--) {
            tmp = a[0];
            a[0] = a[i];
            a[i] = tmp;
            heapify(a, 0 ,i);
        }
    }

    void dsort (Point *array, int n, int sorted)
    {
        Point *a = array;
        Point *b = new Point[n];
        Point *c;

        for (int i = sorted; i < n ; i *= 2) {
            for (int j = 0; j < n; j = j + 2*i) {
                int r = j + i;

                int n1 = (i < n - j) ? i : n - j;
                int n2 = (i < n - r) ? i : n - r;
                n1 = (n1 < 0) ? 0 : n1;
                n2 = (n2 < 0) ? 0 : n2;

                for (int ia = 0, ib = 0, k = 0; k < n1 + n2; k++) {
                    if (ia >= n1)
                        b[j+k] = a[r+ib++];
                    else if (ib >= n2)
                        b[j+k]=a[j+ia++];
                    else if (compare_points(&a[j+ia], &a[r+ib]) < 0)
                        b[j+k]=a[j+ia++];
                    else
                        b[j+k]=a[r+ib++];
                }
            }
            c = a;
            a = b;
            b = c;
        }

        c = a;
        a = b;
        b = c;
        if (b != array) {
            memcpy(array, b, n*sizeof(*array));
            delete [] b;
        } else {
            delete [] a;
        }
    }

    void dhsort(Point *a, int n)
    {
        int nthreads = ceil(n / (double)MAX_HSORT_ELEMS);
        nthreads = nthreads > 4 ? nthreads : 4;
        int tmp = ceil(n / (double)nthreads);
        int elems;
        int offset = 0;
        for (int th = 0; th < nthreads; th++) {
            if (n - offset >= tmp)
                elems = tmp;
            else
                elems = (n - offset > 0) ? n - offset : 0;
            hsort(a + offset, elems);
            offset += elems;
        }
        dsort(a, n, ceil(n / (double)nthreads));
        return;
    }
\end{verbatim}

\chapter{Параллельная сортировка Бэтчера} \label{app:par}
\begin{verbatim}
// File bsort.cpp
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include <time.h>
#include <string.h>
#include <cmath>
#include <pthread.h>

#include "tools.h"
#include "point.h"

#define MIN_THREADS_NUM 4
#define MAX_HSORT_ELEMS 100000

struct PthreadArgs {
    pthread_t tid;
    Point *a;
    int n;
};

void heapify(Point* a, int i, int n)
{
    int imax, l, r;
    Point tmp;
    while (i < n) {
        imax = i;
        l = 2*i + 1;
        r = l + 1;
        if (l < n && (compare_points(&a[l], &a[imax]) > 0))
            imax = l;
        if (r < n && (compare_points(&a[r], &a[imax]) > 0))
            imax = r;
        if (imax == i)
            return;
        tmp = a[i];
        a[i] = a[imax];
        a[imax] = tmp;
        i = imax;
    }
}

void make_heap(Point* a, int n)
{
    for (int i = n/2 - 1; i >= 0; i--)
        heapify(a, i, n);
    }

void hsort(Point *a, int n)
{
    make_heap(a, n);
    Point tmp;
    for (int i = n - 1; i > 0; i--) {
        tmp = a[0];
        a[0] = a[i];
        a[i] = tmp;
        heapify(a, 0 ,i);
    }
}

void* hsort_start(void *arg)
{
    PthreadArgs *args = (PthreadArgs*)arg;
    hsort(args->a, args->n);
    return NULL;
}


void hsort_threads(Point* a, int n, int nthreads)
{
    PthreadArgs *pthread_args = new PthreadArgs[nthreads];
    int tmp = ceil(n / (double)nthreads);
    int elems;
    int offset = 0;
    for (int th = 0; th < nthreads; th++) {
        if (n - offset >= tmp)
            elems = tmp;
        else
            elems = (n - offset > 0) ? n - offset : 0;

        pthread_args[th].a = a + offset;
        pthread_args[th].n = elems;
        pthread_create(&pthread_args[th].tid, NULL, hsort_start,
                       &pthread_args[th]);
        offset += elems;
    }
    for (int th = 0; th < nthreads; th++)
        pthread_join(pthread_args[th].tid, NULL);
    delete [] pthread_args;
}

void dsort (Point *array, int n, int sorted)
{
    Point *a = array;
    Point *b = new Point[n];
    Point *c;

    for (int i = sorted; i < n ; i *= 2) {
        for (int j = 0; j < n; j = j + 2*i) {
            int r = j + i;

            int n1 = (i < n - j) ? i : n - j;
            int n2 = (i < n - r) ? i : n - r;
            n1 = (n1 < 0) ? 0 : n1;
            n2 = (n2 < 0) ? 0 : n2;

            for (int ia = 0, ib = 0, k = 0; k < n1 + n2; k++) {
                if (ia >= n1)
                    b[j+k] = a[r+ib++];
                else if (ib >= n2)
                    b[j+k]=a[j+ia++];
                else if (compare_points(&a[j+ia], &a[r+ib]) < 0)
                    b[j+k]=a[j+ia++];
                else
                    b[j+k]=a[r+ib++];
            }
        }
        c = a;
        a = b;
        b = c;
    }

    c = a;
    a = b;
    b = c;
    if (b != array) {
        memcpy(array, b, n*sizeof(*array));
        delete [] b;
    } else {
        delete [] a;
    }
}

void dhsort(Point *a, int n)
{
    int nthreads = ceil(n / (double)MAX_HSORT_ELEMS);
    nthreads = nthreads > MIN_THREADS_NUM ?
        nthreads : MIN_THREADS_NUM;
    hsort_threads(a, n, nthreads);
    dsort(a, n, ceil(n / (double)nthreads));

    return;
}

void join(std::vector<int> idx_up, int n0, std::vector<int> idx_down, int n1,
          std::vector<comparator> &cmp)
{
    int n = n0 + n1;
    if (n == 1)
        return;

    if (n0 == 1 && n1 == 1) {
        cmp.push_back(comparator(idx_up[0], idx_down[0]));
        return;
    }

    int n0_even = n0/2;
    int n0_odd = n0 - n0_even;
    std::vector<int> idx_up_even(n0_even);
    std::vector<int> idx_up_odd(n0_odd);

    int n1_even = n1/2;
    int n1_odd = n1 - n1_even;
    std::vector<int> idx_down_even(n1_even);
    std::vector<int> idx_down_odd(n1_odd);

    std::vector<int> idx_result;

    int i, i0 = 0, i1 = 0;
    for (i = 0; i < n0; i++)
        if (i%2) {
            idx_up_even[i0] = idx_up[i];
            i0++;
        } else {
            idx_up_odd[i1] = idx_up[i];
            i1++;
        }
    i0 = i1 = 0;
    for (i = 0; i < n1; i++)
        if (i%2) {
            idx_down_even[i0] = idx_down[i];
            i0++;
        } else {
            idx_down_odd[i1] = idx_down[i];
            i1++;
        }

    join(idx_up_odd, n0_odd, idx_down_odd, n1_odd, cmp);
    join(idx_up_even, n0_even, idx_down_even, n1_even, cmp);

    for (i = 0; i < n0; i++)
        idx_result.push_back(idx_up[i]);
    for (i = 0; i < n1; i++)
        idx_result.push_back(idx_down[i]);

    for (int i = 1; i < n - 1; i += 2)
        cmp.push_back(comparator(idx_result[i], idx_result[i + 1]));
}

void sort(std::vector<int>idx, int n, std::vector<comparator> &cmp)
{
    if (n == 1) {
        return;
    }

    int n0 = n/2;
    int n1 = n - n0;

    std::vector<int> idx_up;
    std::vector<int> idx_down;

    int i;
    for (i = 0; i < n0; i++)
        idx_up.push_back(idx[i]);
    for (i = n0; i < n; i++)
        idx_down.push_back(idx[i]);

    sort(idx_up, n0, cmp);
    sort(idx_down, n1, cmp);
    join(idx_up, n0, idx_down, n1, cmp);
}

void make_comparators(int procs, std::vector<comparator> &cmp)
{
    std::vector<int> idx;
    for (int i = 0; i < procs; i++)
        idx.push_back(i);
    sort(idx, procs, cmp);
    return;
}

int main(int argc, char **argv)
{
    int nx, ny;

    // Parsing command line arguments
    if (!check_args(argc, argv, nx, ny))
        return 1;

    MPI_Init(&argc, &argv);

    int rank, procs;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &procs);

    // Comparators network
    std::vector<comparator> cmp;
    make_comparators(procs, cmp);

    // Calculating elems per processor
    int n = nx*ny;
    int fake = n % procs ? (procs - n % procs) : 0;
    int elems = n + fake;
    int proc_elems = elems / procs;

    // Initializing points
    srand(time(NULL) + rank);
    Point *proc_points =
        init_points(n, ny, procs, proc_elems, rank);

    // Sorting
    double start_time = MPI_Wtime();
    dhsort(proc_points, proc_elems);

    // Exchanging elements
    Point *tmp_points = new Point[proc_elems];
    Point *other_points = new Point[proc_elems];
    MPI_Status status;
    MPI_Datatype MPI_POINT = pointType();
    std::vector<comparator>::iterator it;
    for (it = cmp.begin(); it != cmp.end(); it++) {
        if (rank == it->first) {
            MPI_Send(proc_points, proc_elems, MPI_POINT,
                     it->second, 0, MPI_COMM_WORLD);
            MPI_Recv(other_points, proc_elems, MPI_POINT,
                     it->second, 0, MPI_COMM_WORLD, &status);
            int idx = 0;
            int other_idx = 0;
            for (int tmp_idx = 0; tmp_idx < proc_elems; tmp_idx++) {
                Point my = proc_points[idx];
                Point other = other_points[other_idx];
                if (my.coord[0] < other.coord[0]) {
                    tmp_points[tmp_idx] = my;
                    idx++;
                } else {
                    tmp_points[tmp_idx] = other;
                    other_idx++;
                }
            }
            swap_ptr(&proc_points, &tmp_points);
        }

        if (rank == it->second) {
            MPI_Recv(other_points, proc_elems, MPI_POINT,
                     it->first, 0, MPI_COMM_WORLD, &status);
            MPI_Send(proc_points, proc_elems, MPI_POINT,
                     it->first, 0, MPI_COMM_WORLD);
            int idx = proc_elems - 1;
            int other_idx = proc_elems - 1;
            for (int tmp_idx = idx; tmp_idx >= 0; tmp_idx--) {
                Point my = proc_points[idx];
                Point other = other_points[other_idx];
                if (my.coord[0] > other.coord[0]) {
                    tmp_points[tmp_idx] = my;
                    idx--;
                } else {
                    tmp_points[tmp_idx] = other;
                    other_idx--;
                }
            }
            swap_ptr(&proc_points, &tmp_points);
        }
    }

    double end_time = MPI_Wtime();
    double time = end_time - start_time;
    double sort_time = 0;
    MPI_Reduce(&time, &sort_time, 1, MPI_FLOAT,
               MPI_MAX, 0, MPI_COMM_WORLD);

    if (!rank) {
        printf("Elems: %d\nProcs: %d\n", n, procs);
        printf("Sort time: %f sec.\n", sort_time);
    }

    if (argc > 3) {
        write_output(proc_points, proc_elems, argv[3], nx, ny, rank);
    }

    delete [] proc_points;
    MPI_Finalize();
    return 0;
}

\end{verbatim}

\end{document}


